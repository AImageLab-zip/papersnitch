Serviva un modo per ottenere le informazioni relative ai paper da analizzare reperibili sul sito della conferenza. 
La metodologia scelta è stata quella dello scraping data l'indispobilità di API pubbliche per l'estrazione dei dati.
In prima battuta si è pensato di optare per il servizio web di jina, 
    il quale dato un https restituisce una versione compatta e schematizzata in markdown del sito. 

Non restituiva però la totalità delle informazioni necessarie (le metareview), 
pertanto ho optato per utilizzare crawl4ai, una libreria dove è possibile delle configurazioni per lo scraping, 
il che lo rende più flessibile e adatto al nostro scopo.
Ho quindi creato un crawler che estrae le informazioni dei paper che si trovano nella 
pagina principale della conferenza (autori, titolo e link al paper), 
    in seguito viene visitata la pagina del singolo paper per estrarre il resto delle 
    informazioni (abstract, link al codice, dataset, DOI, supplementary material, review, author feedback e metareview).

3/11/2025
Sono riuscito a scaricare i dati di tutti i paper dal sito del MICCAI, 
    manca però il testo completo del pdf (da ragionare su come poi salvarlo su SQL), 
    e ci sono alcuni campi che differiscono come nome da quello che poi dovrebbero avere sul db.
    Tali dati li ho salvati in formato JSON, dovrò trovare un modo per poter caricare su db le varie entry.
4/11/2025
Ho provato ad utilizzare loaddata per caricare i dati sul db, 
    ho avuto problemi con la formattazione del mio JSON e con i campi del database già esistente. 
Ho dovuto quindi accedere al db tramite shell per eliminare le tabelle esistenti e rifare le migrations con django.

5/11/2025
Creato lo script load_data.py per caricare sul db i paper. ancora non funziona perchè c'è ancora da controllare cosa fa il codice creato da claude.
Aggiunti modelli del dataset e del PDFPaper
Aggiunto il cleaning degli autori e dei dataset

6/11/2025
Completato il file load_data.py per caricare i dati sul db. Al momento funziona per poter caricare un paper con i relativi datasets

7/11/2025
Aggiunto pdf al model e aggiunto su load_data il carimento del file sul db, iniziato a guardare come estrarre informazioni dal pdf (libreria pyPDF2)

10/11/2025
Aggiunto supplementary material come filefield e caricato sul modello da load_data.py

11/11/2025
Iniziato a testare llm per l'estrazione delle informazioni dal pdf in read_pdf.py

12/11/2025
Riscontrati problemi nell'installare genai con pipenv (non riusciva a completare il locking).
Iniziato a setuppare uv come nuovo gestore al posto di pipenv. Creato dockerfile per uv.

13/11/2025
Integrato in modo completo (spero) uv nel progetto del docker di django-web

14/11/2025
Scancherato con i volumi di docker per ottenere i file creati dal container nell'host, problema non risolto ma forse non è un problema

TODO:
    - Estrazione dati dal pdf
    - Elaborazioni dati estratti con chatgpt
    - Capire come gestire i file creati dal container (basta vederli dentro il container o è più comodo che anche l'host li abbia?)
    - Ottenere un output decente per le informazioni da estrarre dal pdf con llm (gemini pro could be ok, flash assolutamente no)
   

DOMANDE:
    - Come controllare se il pdf che c'è online sia aggiornato? è possibile che venga cambiato nel tempo?
    - Abbassare i privilegi dell'utente nel docker
    - Creazione di un venv nel docker
    - Come ottenere i link dei dataset dal pdf del paper, si potrebbe avere a disposizione solamente la citazione


